{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64af477f",
   "metadata": {},
   "source": [
    "# VECTOR TILING FOR MAP GENERALIZATION\n",
    "\n",
    "Generalization means creating many copies of a single ‘source of truth’ and tailoring them to the view context\n",
    "\n",
    "Doing this on the fly is labor intensive in a static paper mapping context, especially en masse with variable map scales and contexts (urban, semi-urban, rural) \n",
    "\n",
    "Value in adopting dynamic mapping techniques into the existing processing workflow:\n",
    "\n",
    "- Vector tiling: highly optimized, customizable, retains records’ attributes (versus image tiles)\n",
    "- Customization, filtering, alignment between 'in-house' GRID3 data and third-party sources like OpenStreetMap\n",
    "- API endpoint targets: maps can diff and grow as the sources of truth do\n",
    "- TIPPECANOE: Incredible and fast processing library, somewhat unpredictable w/ emergent effects when processing geometries in a ‘recursive tiled’ manner, but benefits outweigh headaches\n",
    "- Plus: Self-hosting interest (docker/containerization): anxiety after ‘open’ data portals going offline nationwide\n",
    "\n",
    "\n",
    "\n",
    "<!-- - What works?\n",
    "\n",
    "- What doesn't? -->\n",
    "<!-- \n",
    "Note: “Emergent properties” -> usually helpful automation, as long as there are constraints, but need to watch for weirdness and misalignment with what a legend would otherwise say a layer/category should look like\n",
    "\n",
    "(For instance, interpolation between color steps, etc) -->\n",
    "\n",
    "===\n",
    "\n",
    "# Processing pipeline\n",
    "\n",
    "1. **Download** - Fetch Overture Maps and GRID3 (AGOL feature services) data for specified extent (as GeoParquet file)\n",
    "2. **Convert to FlatGeobuf** - Transform GeoParquet to FlatGeobuf for compatibility + efficiency\n",
    "3. **Tile** - Generate PMTiles using tippecanoe with bespoke settings per-layer\n",
    "4. **View** - using maplibre open spec\n",
    "\n",
    "## File formats\n",
    "- **GeoParquet (.parquet)** - Download format (compact, fast queries via \"duckquery\")\n",
    "- **FlatGeobuf (.fgb)** - Convert for optimal tippecanoe library support\n",
    "- **GeoJSON (.geojson)** - Fallback support for small datasets\n",
    "- **pmTiles** - Dynamic vector tiles, served from single static file\n",
    "\n",
    "## CONFIG\n",
    "\n",
    ".env -> config.py imports environment vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e8a8ec",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded environment from repository root: /Users/matthewheaton/GitHub/.env\n",
      "  DATA_DISK = .\n",
      "\n",
      "=== EXTENT FROM ENVIRONMENT (.env) ===\n",
      "  West (lon_min):  19.5\n",
      "  South (lat_min): -12.5\n",
      "  East (lon_max):  30.5\n",
      "  North (lat_max): -3.5\n",
      "  Buffer (degrees): 0\n",
      "\n",
      "  Combined tuple: (19.5, -12.5, 30.5, -3.5)\n",
      "  Buffer: 0.0 degrees\n",
      "\n",
      "=== CONFIGURATION VERIFICATION ===\n",
      "Repository root:       /Users/matthewheaton/GitHub\n",
      "Environment .env:      /Users/matthewheaton/GitHub/.env\n",
      "Environment DATA_DISK: .\n",
      "Config uses:           /Users/matthewheaton/GitHub/basemap\n",
      "PROJECT CONFIGURATION\n",
      "============================================================\n",
      "Project root:        /Users/matthewheaton/GitHub/basemap/1-processing\n",
      "Scripts directory:   /Users/matthewheaton/GitHub/basemap/1-processing/scripts\n",
      "Notebooks directory: /Users/matthewheaton/GitHub/basemap/1-processing/notebooks\n",
      "Data directory:      /Users/matthewheaton/GitHub/basemap/data\n",
      "Scratch directory:   /Users/matthewheaton/GitHub/basemap/data/2-scratch\n",
      "Output directory:    /Users/matthewheaton/GitHub/basemap/data/3-pmtiles\n",
      "Overture data:       /Users/matthewheaton/GitHub/basemap/data/1-input/overture\n",
      "GRID3 data:         /Users/matthewheaton/GitHub/basemap/data/1-input/grid3\n",
      "\n",
      "Processing extent:   (19.5, -12.5, 30.5, -3.5)\n",
      "Buffer degrees:      0.0\n",
      "Area:                99.0000 degree² (~1219779 km²)\n",
      "============================================================\n",
      "\n",
      "✓ Configuration loaded - CONFIG available in all cells\n",
      "\n",
      "⚠️  To change extent: Edit .env file and restart kernel\n",
      "   All downloads, processing, and viewing will use the same extent\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Run this cell first\n",
    "# ============================================================\n",
    "# This cell initializes all configuration and should be run \n",
    "# first. Re-run this cell to reload configuration changes.\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup paths\n",
    "notebook_dir = Path.cwd()\n",
    "processing_dir = notebook_dir.parent  # 1-processing\n",
    "repo_root = processing_dir.parent     # basemap (repository root)\n",
    "\n",
    "# Add processing directory to path\n",
    "if str(processing_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(processing_dir))\n",
    "\n",
    "# Load environment variables from REPOSITORY ROOT (monorepo-wide .env)\n",
    "env_path = repo_root / '.env'\n",
    "load_dotenv(env_path)\n",
    "print(f\"✓ Loaded environment from repository root: {env_path}\")\n",
    "print(f\"  DATA_DISK = {os.environ.get('DATA_DISK', 'not set')}\")\n",
    "\n",
    "# Import configuration (will also load .env via config.py)\n",
    "from config import (\n",
    "    get_config,\n",
    "    ensure_directories,\n",
    "    print_config_summary,\n",
    "    SCRIPTS_DIR,\n",
    "    OUTPUT_DIR,\n",
    "    OVERTURE_DATA_DIR,\n",
    "    GRID3_DATA_DIR,\n",
    "    SCRATCH_DIR,\n",
    ")\n",
    "\n",
    "# Import processing functions\n",
    "from scripts import (\n",
    "    download_overture_data,\n",
    "    convert_file,\n",
    "    convert_parquet_to_fgb,\n",
    "    batch_convert_directory,\n",
    "    process_to_tiles,\n",
    "    create_tilejson,\n",
    "    download_arcgis_data,\n",
    "    batch_download_arcgis_layers,\n",
    ")\n",
    "\n",
    "# Additional libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL CONFIGURATION - Available in all cells below\n",
    "# ============================================================\n",
    "CONFIG = get_config()\n",
    "\n",
    "# ============================================================\n",
    "# EXTENT CONFIGURATION - SINGLE SOURCE OF TRUTH\n",
    "# ============================================================\n",
    "# Extent is now configured in .env file (repository root)\n",
    "# To change the geographic area, edit .env and restart kernel\n",
    "# \n",
    "# Current extent values from .env:\n",
    "print(f\"\\n=== EXTENT FROM ENVIRONMENT (.env) ===\")\n",
    "print(f\"  West (lon_min):  {os.environ.get('EXTENT_WEST', 'not set')}\")\n",
    "print(f\"  South (lat_min): {os.environ.get('EXTENT_SOUTH', 'not set')}\")\n",
    "print(f\"  East (lon_max):  {os.environ.get('EXTENT_EAST', 'not set')}\")\n",
    "print(f\"  North (lat_max): {os.environ.get('EXTENT_NORTH', 'not set')}\")\n",
    "print(f\"  Buffer (degrees): {os.environ.get('EXTENT_BUFFER', 'not set')}\")\n",
    "print(f\"\\n  Combined tuple: {CONFIG['extent']['coordinates']}\")\n",
    "print(f\"  Buffer: {CONFIG['extent']['buffer_degrees']} degrees\")\n",
    "\n",
    "# DO NOT override extent here - edit .env instead!\n",
    "# CONFIG[\"extent\"][\"coordinates\"] is automatically loaded from .env\n",
    "\n",
    "# Processing options (can still be customized here)\n",
    "CONFIG[\"tiling\"][\"input_dirs\"] = [SCRATCH_DIR]  # Read FlatGeobuf files from scratch\n",
    "CONFIG[\"download\"][\"verbose\"] = True\n",
    "CONFIG[\"conversion\"][\"verbose\"] = True\n",
    "CONFIG[\"tiling\"][\"verbose\"] = True\n",
    "CONFIG[\"tiling\"][\"parallel\"] = True\n",
    "\n",
    "# Create directories and verify\n",
    "ensure_directories()\n",
    "\n",
    "# Verification\n",
    "print(\"\\n=== CONFIGURATION VERIFICATION ===\")\n",
    "print(f\"Repository root:       {repo_root}\")\n",
    "print(f\"Environment .env:      {env_path}\")\n",
    "print(f\"Environment DATA_DISK: {os.environ.get('DATA_DISK', 'NOT SET')}\")\n",
    "print(f\"Config uses:           {CONFIG['paths']['data_dir'].parent}\")\n",
    "\n",
    "print_config_summary(CONFIG)\n",
    "print(\"\\n✓ Configuration loaded - CONFIG available in all cells\")\n",
    "print(\"\\n⚠️  To change extent: Edit .env file and restart kernel\")\n",
    "print(\"   All downloads, processing, and viewing will use the same extent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea97440",
   "metadata": {},
   "source": [
    "## 2. Download Overture Data with DuckDB \n",
    "\n",
    "Use the `downloadOverture.py` module to fetch geospatial data from Overture Maps\n",
    "\n",
    "e.g., replace periodic geofabrik OSM ~shapefile~ fetches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b170545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Overture Maps data\n",
    "print(\"=== STEP 1: DOWNLOADING OVERTURE DATA ===\")\n",
    "download_results = download_overture_data(\n",
    "    extent=CONFIG[\"extent\"][\"coordinates\"],\n",
    "    buffer_degrees=CONFIG[\"extent\"][\"buffer_degrees\"],\n",
    "    template_path=str(CONFIG[\"paths\"][\"template_path\"]),\n",
    "    verbose=CONFIG[\"download\"][\"verbose\"],\n",
    "    project_root=str(CONFIG[\"paths\"][\"project_root\"]),\n",
    "    overture_data_dir=str(CONFIG[\"paths\"][\"overture_data_dir\"])\n",
    ")\n",
    "\n",
    "print(f\"Download completed: {download_results['success']}\")\n",
    "print(f\"Sections processed: {download_results['processed_sections']}\")\n",
    "if download_results[\"errors\"]:\n",
    "    print(f\"Errors encountered: {len(download_results['errors'])}\")\n",
    "    for error in download_results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f11f7c",
   "metadata": {},
   "source": [
    "## 2a. Download ArcGIS Feature Server Data\n",
    "\n",
    "Download geospatial data from hosted ArcGIS Feature Server REST API endpoints - can include any esri-hosted data as endpoint\n",
    "\n",
    "- **Automatic pagination** - Handles ArcGIS's 1000-2000 feature limit per request\n",
    "- **Spatial filtering** - Apply bounding box filter to download only features in aoi\n",
    "- **formats** - Download as GeoJSON or directly convert to FlatGeobuf\n",
    "- **Batch processing** - Download multiple layers with one function call\n",
    "\n",
    "### GRID3 DRC Layers\n",
    "- https://services3.arcgis.com/BU6Aadhn6tbBEdyk/arcgis/rest/services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0085a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ArcGIS Feature Server data (optional - skip if not needed)\n",
    "print(\"=== STEP 2a: DOWNLOADING ARCGIS DATA (OPTIONAL) ===\")\n",
    "print(f\"Using extent from CONFIG: {CONFIG['extent']['coordinates']}\")\n",
    "print(f\"  Longitude: {CONFIG['extent']['coordinates'][0]} to {CONFIG['extent']['coordinates'][2]}\")\n",
    "print(f\"  Latitude: {CONFIG['extent']['coordinates'][1]} to {CONFIG['extent']['coordinates'][3]}\")\n",
    "\n",
    "# Define ArcGIS layers to download\n",
    "# Uncomment and customize the layers you need\n",
    "arcgis_layers = [\n",
    "    {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/arcgis/rest/services/GRID3_COD_health_zones_v7_0/FeatureServer/0',\n",
    "        'name': 'health_zones',\n",
    "        'where': '1=1'  # Download all features (can add SQL filter here)\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/arcgis/rest/services/GRID3_COD_Settlement_Extents_v3_1/FeatureServer/0',\n",
    "        'name': 'settlement_extents',\n",
    "        'where': '1=1'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/ArcGIS/rest/services/GRID3_COD_health_areas_v7_0/FeatureServer/0',\n",
    "        'name': 'health_areas',\n",
    "        'where': '1=1'\n",
    "    },\n",
    "        {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/ArcGIS/rest/services/GRID3_COD_settlement_names_v7_0/FeatureServer/0',\n",
    "        'name': 'settlement_names',\n",
    "        'where': '1=1'  \n",
    "    },\n",
    "    {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/ArcGIS/rest/services/COD_GRID3_health_facilities_v7_0/FeatureServer/0',\n",
    "        'name': 'health_facilities',\n",
    "        'where': '1=1'\n",
    "    },\n",
    "    {\n",
    "        'url': 'https://services3.arcgis.com/BU6Aadhn6tbBEdyk/ArcGIS/rest/services/GRID3_COD_religious_centers_v1_0/FeatureServer/0',\n",
    "        'name': 'religious_centers',\n",
    "        'where': '1=1'\n",
    "    },\n",
    "]\n",
    "\n",
    "# Download layers using the SAME EXTENT as Overture data\n",
    "# This ensures all data layers align spatially\n",
    "if arcgis_layers:\n",
    "    arcgis_results = batch_download_arcgis_layers(\n",
    "        layer_configs=arcgis_layers,\n",
    "        output_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Save directly to scratch for tiling\n",
    "        extent=CONFIG[\"extent\"][\"coordinates\"],  # ← SAME EXTENT as Overture downloads\n",
    "        output_format=\"fgb\",  # Use FlatGeobuf for optimal tiling performance\n",
    "        verbose=CONFIG[\"download\"][\"verbose\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nArcGIS Download Summary:\")\n",
    "    print(f\"  Total layers: {arcgis_results['total_layers']}\")\n",
    "    print(f\"  Successful: {arcgis_results['successful']}\")\n",
    "    print(f\"  Failed: {arcgis_results['failed']}\")\n",
    "    \n",
    "    for layer in arcgis_results['layers']:\n",
    "        if layer['success']:\n",
    "            print(f\"  ✓ {layer['name']}: {layer['feature_count']:,} features\")\n",
    "        else:\n",
    "            print(f\"  ✗ {layer['name']}: {layer.get('error', 'Unknown error')}\")\n",
    "else:\n",
    "    print(\"No ArcGIS layers configured. Edit arcgis_layers list above to download data.\")\n",
    "    print(f\"\\nNote: When configured, downloads will use extent: {CONFIG['extent']['coordinates']}\")\n",
    "\n",
    "# Option 2: Download a single layer (alternative approach)\n",
    "# Also uses the same extent from CONFIG\n",
    "# Uncomment and customize as needed:\n",
    "# single_layer_result = download_arcgis_data(\n",
    "#     service_url='https://services3.arcgis.com/BU6Aadhn6tbBEdyk/arcgis/rest/services/GRID3_COD_health_zones_v7_0/FeatureServer/0',\n",
    "#     output_path=str(CONFIG[\"paths\"][\"scratch_dir\"] / \"health_zones.fgb\"),\n",
    "#     extent=CONFIG[\"extent\"][\"coordinates\"],  # ← Uses CONFIG extent\n",
    "#     output_format=\"fgb\",\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3aabc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate centroids for administrative boundary labels\n",
    "print(\"=== STEP 2b: GENERATING CENTROIDS FOR ADMINISTRATIVE LABELS ===\")\n",
    "\n",
    "# Import the centroid generation function\n",
    "from scripts import batch_generate_centroids\n",
    "\n",
    "# Define which layers need centroids for label positioning\n",
    "# These correspond to the administrative boundary layers\n",
    "layers_for_centroids = [\n",
    "    'health_zones',   # Health zone polygons -> health_zones_centroids\n",
    "    'health_areas',   # Health area polygons -> health_areas_centroids\n",
    "]\n",
    "\n",
    "# Generate centroids for specified layers\n",
    "centroid_results = batch_generate_centroids(\n",
    "    input_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Where polygon FGB files are\n",
    "    output_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Save centroids alongside polygons\n",
    "    layers=layers_for_centroids,                     # Only process these layers\n",
    "    suffix='_centroids',                             # Output: layer_name_centroids.fgb\n",
    "    verbose=CONFIG[\"download\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nCentroid Generation Summary:\")\n",
    "print(f\"  Total layers: {centroid_results['total_layers']}\")\n",
    "print(f\"  Successful: {centroid_results['successful']}\")\n",
    "print(f\"  Failed: {centroid_results['failed']}\")\n",
    "\n",
    "for layer in centroid_results['layers']:\n",
    "    if layer['success']:\n",
    "        output_name = Path(layer['output_file']).name\n",
    "        print(f\"  ✓ {output_name}: {layer['feature_count']:,} centroids\")\n",
    "    else:\n",
    "        print(f\"  ✗ {Path(layer['input_file']).name}: {layer.get('error', 'Unknown error')}\")\n",
    "\n",
    "# List all files ready for tiling (including new centroids)\n",
    "if CONFIG[\"paths\"][\"scratch_dir\"].exists():\n",
    "    all_fgb_files = sorted(CONFIG[\"paths\"][\"scratch_dir\"].glob(\"*.fgb\"))\n",
    "    centroid_files = [f for f in all_fgb_files if '_centroids' in f.name]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Files ready for tiling:\")\n",
    "    print(f\"  Total FlatGeobuf files: {len(all_fgb_files)}\")\n",
    "    print(f\"  Centroid files: {len(centroid_files)}\")\n",
    "    if centroid_files:\n",
    "        for f in centroid_files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    print(f\"  Location: {CONFIG['paths']['scratch_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee605ffe",
   "metadata": {},
   "source": [
    "## 2b. Generate Centroids for Administrative Polygons\n",
    "\n",
    "Generate interior centroid points for health zones and health areas. These will be used for single-label-per-polygon rendering in the map viewer (interior labels at lower zoom levels).\n",
    "\n",
    "**Why centroids?**\n",
    "- Guarantees one label per polygon (no duplicates across tile boundaries)\n",
    "- `representative_point()` ensures label is always inside the polygon\n",
    "- Preserves all attributes for label content\n",
    "- Separate point layer is more efficient than point-based symbol placement on polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ac49f",
   "metadata": {},
   "source": [
    "## 2c. Generate Centerlines for Water Polygons\n",
    "\n",
    "Generate centerline features for polygonal water bodies (lakes, reservoirs, etc.). These will be used for placing labels along the natural axis of elongated water features.\n",
    "\n",
    "**Why centerlines?**\n",
    "- Better label placement for elongated water features (lakes, reservoirs)\n",
    "- Creates linear features along the medial axis of polygons\n",
    "- Labels follow the natural orientation of the water body\n",
    "- Uses Voronoi-based skeleton algorithm for accurate centerline extraction\n",
    "- Preserves all attributes for label content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceaa205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate centerlines for water feature labels\n",
    "print(\"=== STEP 2c: GENERATING CENTERLINES FOR WATER LABELS ===\")\n",
    "\n",
    "# Import the centerline generation function\n",
    "from scripts import batch_generate_centerlines\n",
    "\n",
    "# Define which layers need centerlines for label positioning\n",
    "# Typically used for elongated water bodies like lakes and reservoirs\n",
    "layers_for_centerlines = [\n",
    "    'water',  # Water polygons -> water_centerlines\n",
    "]\n",
    "\n",
    "# Generate centerlines for specified layers\n",
    "centerline_results = batch_generate_centerlines(\n",
    "    input_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Where polygon FGB files are\n",
    "    output_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Save centerlines alongside polygons\n",
    "    layers=layers_for_centerlines,                   # Only process these layers\n",
    "    suffix='_centerlines',                           # Output: layer_name_centerlines.fgb\n",
    "    simplify_tolerance=5.0,                          # N meters simplification\n",
    "    border_density=20,                            # Increase this for winding rivers (default is 100)\n",
    "    verbose=CONFIG[\"download\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "print(f\"  Total layers: {centerline_results['total_layers']}\")\n",
    "print(f\"  Successful: {centerline_results['successful']}\")\n",
    "print(f\"  Failed: {centerline_results['failed']}\")\n",
    "\n",
    "for layer in centerline_results['layers']:\n",
    "    if layer['success']:\n",
    "        output_name = Path(layer['output_file']).name\n",
    "        print(f\"  ✓ {output_name}: {layer['feature_count']:,} centerlines\")\n",
    "    else:\n",
    "        print(f\"  ✗ {Path(layer['input_file']).name}: {layer.get('error', 'Unknown error')}\")\n",
    "\n",
    "# List all processed files ready for tiling\n",
    "if CONFIG[\"paths\"][\"scratch_dir\"].exists():\n",
    "    all_fgb_files = sorted(CONFIG[\"paths\"][\"scratch_dir\"].glob(\"*.fgb\"))\n",
    "    centroid_files = [f for f in all_fgb_files if '_centroids' in f.name]\n",
    "    centerline_files = [f for f in all_fgb_files if '_centerlines' in f.name]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processed geometry files ready for tiling:\")\n",
    "    print(f\"  Total FlatGeobuf files: {len(all_fgb_files)}\")\n",
    "    print(f\"  Centroid files: {len(centroid_files)}\")\n",
    "    if centroid_files:\n",
    "        for f in centroid_files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    print(f\"  Centerline files: {len(centerline_files)}\")\n",
    "    if centerline_files:\n",
    "        for f in centerline_files:\n",
    "            print(f\"    - {f.name}\")\n",
    "    print(f\"  Location: {CONFIG['paths']['scratch_dir']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a64ae5",
   "metadata": {},
   "source": [
    "### ArcGIS Feature Server Downloads\n",
    "\n",
    "**Finding Feature Server URLs:**\n",
    "1. Browse your organization's ArcGIS REST Services Directory\n",
    "2. Navigate to a specific layer (e.g., FeatureServer/0, FeatureServer/1)\n",
    "3. Copy the full URL up to and including the layer number\n",
    "4. The script will automatically append `/query` and handle parameters\n",
    "\n",
    "**Spatial Filtering:**\n",
    "- The `extent` parameter filters features to your bounding box (saves bandwidth & time)\n",
    "- For global layers, omit `extent=None` to download all features\n",
    "- Extent uses WGS84 coordinates: `(lon_min, lat_min, lon_max, lat_max)`\n",
    "\n",
    "**Attribute Filtering:**\n",
    "- Use `where` clause for SQL-based filtering: `'population > 10000'`\n",
    "- Default `'1=1'` downloads all features\n",
    "\n",
    "**Output Formats:**\n",
    "- `\"fgb\"` (FlatGeobuf) - Recommended for direct tiling (streaming, indexed)\n",
    "- `\"geojson\"` - more flexible, less optimal\n",
    "\n",
    "\n",
    "- Large datasets (>100k features) automatically use pagination\n",
    "- Downloads directly to scratch directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7860e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files were created during download\n",
    "print(\"=== CHECKING DOWNLOADED FILES ===\")\n",
    "\n",
    "# Check Overture Maps downloads (GeoParquet)\n",
    "overture_files = []\n",
    "search_dirs = [CONFIG[\"paths\"][\"data_dir\"], CONFIG[\"paths\"][\"overture_data_dir\"]]\n",
    "\n",
    "for data_dir in search_dirs:\n",
    "    if data_dir.exists():\n",
    "        for pattern in CONFIG[\"download\"][\"output_formats\"]:\n",
    "            files = list(data_dir.glob(pattern))\n",
    "            overture_files.extend(files)\n",
    "\n",
    "print(f\"\\nOverture Maps: {len(overture_files)} files\")\n",
    "for file in sorted(overture_files):\n",
    "    file_size = file.stat().st_size / 1024 / 1024  # Size in MB\n",
    "    print(f\"  {file.name} ({file_size:.1f} MB)\")\n",
    "\n",
    "# Check ArcGIS downloads (FlatGeobuf in scratch)\n",
    "arcgis_files = []\n",
    "if CONFIG[\"paths\"][\"scratch_dir\"].exists():\n",
    "    arcgis_files = list(CONFIG[\"paths\"][\"scratch_dir\"].glob(\"*.fgb\"))\n",
    "    # Filter out converted Overture files (they have matching .parquet names)\n",
    "    overture_names = {f.stem for f in overture_files}\n",
    "    arcgis_files = [f for f in arcgis_files if f.stem not in overture_names]\n",
    "\n",
    "print(f\"\\nArcGIS Feature Server: {len(arcgis_files)} files\")\n",
    "for file in sorted(arcgis_files):\n",
    "    file_size = file.stat().st_size / 1024 / 1024  # Size in MB\n",
    "    print(f\"  {file.name} ({file_size:.1f} MB)\")\n",
    "\n",
    "# Display overall statistics\n",
    "all_files = overture_files + arcgis_files\n",
    "if all_files:\n",
    "    total_size_mb = sum(f.stat().st_size for f in all_files) / 1024 / 1024\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Total downloaded: {len(all_files)} files ({total_size_mb:.1f} MB)\")\n",
    "    print(f\"  Overture Maps (GeoParquet): {len(overture_files)} files\")\n",
    "    print(f\"  ArcGIS (FlatGeobuf): {len(arcgis_files)} files\")\n",
    "else:\n",
    "    print(\"\\nNo files found. Run download steps above first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a89d40a",
   "metadata": {},
   "source": [
    "## 3. Convert GeoParquet to FlatGeobuf\n",
    "\n",
    "Convert downloaded Overture GeoParquet files to FlatGeobuf format for tippecanoe compatibility\n",
    "\n",
    "**Note**: ArcGIS data was already downloaded as FlatGeobuf in Step 2a, so this step only processes Overture Maps data. Both sources will coexist in the scratch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that downloaded data extent matches configured extent\n",
    "print(\"=== EXTENT VERIFICATION ===\")\n",
    "\n",
    "# Get configured extent from CONFIG (loaded from .env)\n",
    "config_extent = CONFIG[\"extent\"][\"coordinates\"]\n",
    "config_west, config_south, config_east, config_north = config_extent\n",
    "\n",
    "print(f\"\\n1. Configured Extent (from .env):\")\n",
    "print(f\"   West:  {config_west:>10.6f}\")\n",
    "print(f\"   South: {config_south:>10.6f}\")\n",
    "print(f\"   East:  {config_east:>10.6f}\")\n",
    "print(f\"   North: {config_north:>10.6f}\")\n",
    "\n",
    "# Check actual extent of downloaded files\n",
    "import subprocess\n",
    "\n",
    "def get_fgb_extent(file_path):\n",
    "    \"\"\"Get extent from FlatGeobuf file using ogrinfo\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ogrinfo', '-al', '-so', str(file_path)],\n",
    "            capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Extent:' in line:\n",
    "                # Parse: Extent: (20.294878, -7.704176) - (23.705435, -3.795773)\n",
    "                parts = line.split(':')[1].strip()\n",
    "                parts = parts.replace('(', '').replace(')', '').replace(' - ', ',')\n",
    "                coords = [float(x.strip()) for x in parts.split(',')]\n",
    "                return tuple(coords)  # (west, south, east, north)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Check key files\n",
    "check_files = ['buildings.fgb', 'roads.fgb', 'water.fgb']\n",
    "mismatches = []\n",
    "\n",
    "print(f\"\\n2. Downloaded Data Extents:\")\n",
    "for filename in check_files:\n",
    "    fgb_path = CONFIG[\"paths\"][\"scratch_dir\"] / filename\n",
    "    if fgb_path.exists():\n",
    "        extent = get_fgb_extent(fgb_path)\n",
    "        if extent:\n",
    "            west, south, east, north = extent\n",
    "            print(f\"\\n   {filename}:\")\n",
    "            print(f\"     West:  {west:>10.6f}\")\n",
    "            print(f\"     South: {south:>10.6f}\")\n",
    "            print(f\"     East:  {east:>10.6f}\")\n",
    "            print(f\"     North: {north:>10.6f}\")\n",
    "            \n",
    "            # Check if extents match (within 0.5 degree tolerance for tile snapping)\n",
    "            tolerance = 0.5\n",
    "            west_match = abs(west - config_west) < tolerance\n",
    "            south_match = abs(south - config_south) < tolerance\n",
    "            east_match = abs(east - config_east) < tolerance\n",
    "            north_match = abs(north - config_north) < tolerance\n",
    "            \n",
    "            if not (west_match and south_match and east_match and north_match):\n",
    "                mismatches.append({\n",
    "                    'file': filename,\n",
    "                    'data_extent': extent,\n",
    "                    'config_extent': config_extent\n",
    "                })\n",
    "    else:\n",
    "        print(f\"\\n   {filename}: Not found\")\n",
    "\n",
    "# Report results\n",
    "print(f\"\\n{'='*60}\")\n",
    "if mismatches:\n",
    "    print(\"EXTENT MISMATCH DETECTED!\")\n",
    "    print(f\"\\n   {len(mismatches)} file(s) have different extents than configured.\")\n",
    "    print(f\"\\n   SOLUTION:\")\n",
    "    print(f\"   1. If you want the data in the downloaded files:\")\n",
    "    print(f\"      - Update EXTENT_* values in .env to match data extent\")\n",
    "    print(f\"      - Restart kernel and re-run cells\")\n",
    "    print(f\"\\n   2. If you want the extent currently in .env:\")\n",
    "    print(f\"      - Delete files: {CONFIG['paths']['scratch_dir']}\")\n",
    "    print(f\"      - Delete files: {CONFIG['paths']['overture_data_dir']}\")\n",
    "    print(f\"      - Re-run download cells (Step 1 and 2a)\")\n",
    "    print(f\"\\n   DO NOT proceed to tiling until extents match!\")\n",
    "else:\n",
    "    print(\"EXTENT VERIFICATION PASSED\")\n",
    "    print(f\"\\n   All downloaded data matches the configured extent.\")\n",
    "    print(f\"   Safe to proceed with tile generation.\")\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030782fb",
   "metadata": {},
   "source": [
    "## 2.5b. Verify Extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GeoParquet files to FlatGeobuf for optimal tiling performance\n",
    "print(\"=== STEP 3: CONVERTING OVERTURE GEOPARQUET TO FLATGEOBUF ===\")\n",
    "print(\"Note: ArcGIS data already in FlatGeobuf format (from Step 2a)\")\n",
    "\n",
    "# Get list of existing ArcGIS FlatGeobuf files to avoid overwriting\n",
    "existing_fgb_files = set()\n",
    "if CONFIG[\"paths\"][\"scratch_dir\"].exists():\n",
    "    existing_fgb_files = {f.stem for f in CONFIG[\"paths\"][\"scratch_dir\"].glob(\"*.fgb\")}\n",
    "    if existing_fgb_files:\n",
    "        print(f\"Preserving {len(existing_fgb_files)} existing ArcGIS FlatGeobuf files:\")\n",
    "        for name in sorted(existing_fgb_files):\n",
    "            print(f\"  - {name}.fgb\")\n",
    "\n",
    "# Convert Overture GeoParquet files to FlatGeobuf\n",
    "# These will be added alongside ArcGIS files in the scratch directory\n",
    "fgb_results = batch_convert_directory(\n",
    "    input_dir=str(CONFIG[\"paths\"][\"overture_data_dir\"]),\n",
    "    output_dir=str(CONFIG[\"paths\"][\"scratch_dir\"]),  # Save FGB files to scratch directory\n",
    "    pattern=CONFIG[\"fgb_conversion\"][\"input_pattern\"],\n",
    "    overwrite=CONFIG[\"fgb_conversion\"][\"overwrite\"],\n",
    "    verbose=CONFIG[\"fgb_conversion\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nOverture Conversion Summary:\")\n",
    "print(f\"  Converted: {fgb_results['converted']} files\")\n",
    "print(f\"  Skipped:   {fgb_results['skipped']} files (already exist)\")\n",
    "print(f\"  Errors:    {len(fgb_results['errors'])} files\")\n",
    "\n",
    "if fgb_results['errors']:\n",
    "    print(\"\\nErrors encountered:\")\n",
    "    for error in fgb_results['errors']:\n",
    "        print(f\"  - {error['file']}: {error['error']}\")\n",
    "\n",
    "# Count total FlatGeobuf files ready for tiling\n",
    "if CONFIG[\"paths\"][\"scratch_dir\"].exists():\n",
    "    all_fgb_files = list(CONFIG[\"paths\"][\"scratch_dir\"].glob(\"*.fgb\"))\n",
    "    overture_fgb_count = len([f for f in all_fgb_files if f.stem not in existing_fgb_files])\n",
    "    arcgis_fgb_count = len(existing_fgb_files)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"✓ All FlatGeobuf files ready for tiling\")\n",
    "    print(f\"  Location: {CONFIG['paths']['scratch_dir']}\")\n",
    "    print(f\"  Total files: {len(all_fgb_files)}\")\n",
    "    print(f\"    - Overture (converted): {overture_fgb_count}\")\n",
    "    print(f\"    - ArcGIS (direct): {arcgis_fgb_count}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ No FlatGeobuf files found in scratch directory\")\n",
    "    if fgb_results['skipped'] > 0:\n",
    "        print(f\"All {fgb_results['skipped']} Overture files already converted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60968d93",
   "metadata": {},
   "source": [
    "## 4. Process FlatGeobuf to PMTiles\n",
    "\n",
    "Use the `runCreateTiles.py` module to convert FlatGeobuf files to PMTiles using custom tippecanoe queries from tippecanoe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a575ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 4: PROCESSING TO PMTILES ===\n",
      "=== PROCESSING TO TILES ===\n",
      "Found 2 files to process:\n",
      "  water.fgb (FlatGeobuf)\n",
      "  water_centerlines.fgb (FlatGeobuf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/2 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using template settings for water.fgb (7 options)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 1/2 [00:02<00:02,  2.87s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ water_centerlines.fgb -> /Users/matthewheaton/GitHub/basemap/data/3-pmtiles/water_centerlines.pmtiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 2/2 [00:22<00:00, 11.08s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ water.fgb -> /Users/matthewheaton/GitHub/basemap/data/3-pmtiles/water.pmtiles\n",
      "\n",
      "=== TILE PROCESSING COMPLETE ===\n",
      "Processed: 2/2 files\n",
      "\n",
      "✓ Successfully generated 2 PMTiles:\n",
      "  buildings.pmtiles (379.1 MB)\n",
      "  health_areas.pmtiles (61.9 MB)\n",
      "  health_areas_centroids.pmtiles (17.4 MB)\n",
      "  health_facilities.pmtiles (20.8 MB)\n",
      "  health_zones.pmtiles (17.8 MB)\n",
      "  health_zones_centroids.pmtiles (1.1 MB)\n",
      "  infrastructure.pmtiles (5.8 MB)\n",
      "  land_cover.pmtiles (834.7 MB)\n",
      "  land_residential.pmtiles (12.3 MB)\n",
      "  land_use.pmtiles (6.7 MB)\n",
      "  religious_centers.pmtiles (11.6 MB)\n",
      "  roads.pmtiles (287.6 MB)\n",
      "  settlement_extents.pmtiles (62.0 MB)\n",
      "  settlement_names.pmtiles (28.3 MB)\n",
      "  water.pmtiles (35.7 MB)\n",
      "  water_centerlines.pmtiles (1.8 MB)\n",
      "\n",
      "Total PMTiles size: 1784.5 MB\n",
      "Files location: /Users/matthewheaton/GitHub/basemap/data/3-pmtiles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Process all geospatial files to PMTiles\n",
    "print(\"=== STEP 4: PROCESSING TO PMTILES ===\")\n",
    "\n",
    "# Process all downloaded and converted files to PMTiles using CONFIG settings\n",
    "# Now supports: GeoJSON, GeoJSONSeq, and GeoParquet formats\n",
    "tiling_results = process_to_tiles(\n",
    "    extent=CONFIG[\"extent\"][\"coordinates\"],\n",
    "    input_dirs=[str(d) for d in CONFIG[\"tiling\"][\"input_dirs\"]],  # Convert Path objects to strings\n",
    "    filter_pattern=CONFIG[\"tiling\"][\"filter_pattern\"],  # Pass filter pattern from CONFIG\n",
    "    output_dir=str(CONFIG[\"tiling\"][\"output_dir\"]),  # Use explicit output directory from CONFIG\n",
    "    parallel=CONFIG[\"tiling\"][\"parallel\"],\n",
    "    verbose=CONFIG[\"tiling\"][\"verbose\"]\n",
    ")\n",
    "\n",
    "# print(f\"Tiling completed: {tiling_results['success']}\")\n",
    "# print(f\"Files processed: {len(tiling_results['processed_files'])}/{tiling_results['total_files']}\")\n",
    "\n",
    "if tiling_results[\"errors\"]:\n",
    "    print(f\"Errors encountered: {len(tiling_results['errors'])}\")\n",
    "    for error in tiling_results[\"errors\"]:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Display generated PMTiles files\n",
    "if tiling_results[\"processed_files\"]:\n",
    "    print(f\"\\n✓ Successfully generated {len(tiling_results['processed_files'])} PMTiles:\")\n",
    "    \n",
    "    pmtiles_files = list(CONFIG[\"paths\"][\"tile_dir\"].glob(\"*.pmtiles\"))\n",
    "    \n",
    "    total_size_mb = 0\n",
    "    for pmtile in sorted(pmtiles_files):\n",
    "        size_mb = pmtile.stat().st_size / 1024 / 1024\n",
    "        total_size_mb += size_mb\n",
    "        print(f\"  {pmtile.name} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\nTotal PMTiles size: {total_size_mb:.1f} MB\")\n",
    "    print(f\"Files location: {CONFIG['paths']['tile_dir']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo PMTiles files were generated. Check the errors above.\")\n",
    "    print(f\"Make sure you have geospatial files (GeoJSON/GeoJSONSeq/GeoParquet) in: {[str(d) for d in CONFIG['tiling']['input_dirs']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f46e1",
   "metadata": {},
   "source": [
    "## 5. Create TileJSON Metadata for map viewer\n",
    "\n",
    "- **Set bounds and zoom levels**\n",
    "- **PMTiles URL references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57093621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create TileJSON metadata for MapLibre integration\n",
    "print(\"=== STEP 5: CREATING TILEJSON METADATA ===\")\n",
    "\n",
    "# Check if PMTiles files exist in the configured tile directory\n",
    "pmtiles_files = list(CONFIG[\"paths\"][\"tile_dir\"].glob(\"*.pmtiles\"))\n",
    "\n",
    "if pmtiles_files:\n",
    "    print(f\"Found {len(pmtiles_files)} PMTiles files, creating TileJSON...\")\n",
    "    \n",
    "    try:\n",
    "        tilejson = create_tilejson(\n",
    "            tile_dir=str(CONFIG[\"paths\"][\"tile_dir\"]),  # Explicitly pass tile directory\n",
    "            extent=CONFIG[\"extent\"][\"coordinates\"],  # Pass extent from CONFIG\n",
    "            output_file=str(CONFIG[\"paths\"][\"tile_dir\"] / \"tilejson.json\")  # Explicitly pass output file path\n",
    "        )\n",
    "        \n",
    "        print(\"✓ TileJSON created successfully\")\n",
    "        print(f\"  Bounds: {tilejson['bounds']}\")\n",
    "        print(f\"  Zoom range: {tilejson['minzoom']} - {tilejson['maxzoom']}\")\n",
    "        print(f\"  Vector layers: {len(tilejson['vector_layers'])}\")\n",
    "        print(f\"  Output file: {CONFIG['paths']['tile_dir'] / 'tilejson.json'}\")\n",
    "        \n",
    "        # Show a summary of all output files\n",
    "        print(f\"\\nComplete output summary:\")\n",
    "        total_size_mb = 0\n",
    "        for pmtile in sorted(pmtiles_files):\n",
    "            size_mb = pmtile.stat().st_size / 1024 / 1024\n",
    "            total_size_mb += size_mb\n",
    "            print(f\"  {pmtile.name} ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        print(f\"  tilejson.json\")\n",
    "        print(f\"\\nTotal PMTiles size: {total_size_mb:.1f} MB\")\n",
    "        print(f\"All files location: {CONFIG['paths']['tile_dir']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ TileJSON creation failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"No PMTiles files found in output directory.\")\n",
    "    print(f\"Expected location: {CONFIG['paths']['tile_dir']}\")\n",
    "    print(\"Run Step 4 first to generate PMTiles files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755057e5",
   "metadata": {},
   "source": [
    "## 6. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Step Testing and Validation\n",
    "\n",
    "print(\"INDIVIDUAL STEP TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Test downloadOverture.py standalone:\")\n",
    "print(\"python scripts/downloadOverture.py --extent='27.0,-8.0,30.5,-2.0' --buffer=0\")\n",
    "\n",
    "print(\"\\n2. Test downloadArcGIS.py standalone:\")\n",
    "print(\"python scripts/downloadArcGIS.py \\\\\")\n",
    "print(\"  'https://services3.arcgis.com/.../FeatureServer/0' \\\\\")\n",
    "print(\"  output.fgb --extent='27.0,-8.0,30.5,-2.0' --format=fgb\")\n",
    "\n",
    "print(\"\\n3. Test runCreateTiles.py standalone:\")\n",
    "print(\"python scripts/runCreateTiles.py --extent='27.0,-8.0,30.5,-2.0' --create-tilejson\")\n",
    "\n",
    "print(\"\\n4. Test individual steps in this notebook:\")\n",
    "print(\"   - Step 1: Download Overture data\")\n",
    "print(\"   - Step 2a: Download ArcGIS data (optional)\")\n",
    "print(\"   - Step 2b: Check downloaded files\")\n",
    "print(\"   - Step 2.5: Convert GeoParquet to FlatGeobuf\")\n",
    "print(\"   - Step 4: Process to PMTiles\")\n",
    "print(\"   - Step 5: Create TileJSON\")\n",
    "\n",
    "print(\"\\n5. Validate outputs using CONFIG paths:\")\n",
    "print(f\"   - Overture GeoParquet: {CONFIG['paths']['overture_data_dir']}\")\n",
    "print(f\"   - FlatGeobuf files: {CONFIG['paths']['scratch_dir']}\")\n",
    "print(f\"   - PMTiles: {CONFIG['paths']['tile_dir']}\")\n",
    "print(f\"   - TileJSON metadata: {CONFIG['paths']['tile_dir']}/tilejson.json\")\n",
    "\n",
    "# Configuration validation using centralized CONFIG\n",
    "print(\"\\nCURRENT CONFIGURATION VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Extent: {CONFIG['extent']['coordinates']}\")\n",
    "print(f\"Buffer: {CONFIG['extent']['buffer_degrees']} degrees\")\n",
    "print(f\"Tile output directory: {CONFIG['paths']['tile_dir']}\")\n",
    "print(f\"Scratch directory (FlatGeobuf): {CONFIG['paths']['scratch_dir']}\")\n",
    "print(f\"Input directories for tiling: {[str(d) for d in CONFIG['tiling']['input_dirs']]}\")\n",
    "\n",
    "# Area calculation using CONFIG\n",
    "extent = CONFIG['extent']['coordinates']\n",
    "area = (extent[2] - extent[0]) * (extent[3] - extent[1])\n",
    "print(f\"Processing area: {area:.2f} degree² ({area * 111**2:.0f} km²)\")\n",
    "\n",
    "# Check directory status\n",
    "print(f\"\\nDIRECTORY STATUS\")\n",
    "print(\"=\" * 30)\n",
    "for path_name, path_obj in CONFIG['paths'].items():\n",
    "    if path_name.endswith('_dir'):\n",
    "        status = \"exists\" if path_obj.exists() else \"missing\"\n",
    "        file_count = len(list(path_obj.glob(\"*\"))) if path_obj.exists() else 0\n",
    "        print(f\"{path_name}: {status} ({file_count} files)\")\n",
    "\n",
    "print(\"\\nPERFORMANCE OPTIMIZATION TIPS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. For large areas (current: {area:.2f} degree²):\")\n",
    "print(f\"   - Current buffer: {CONFIG['extent']['buffer_degrees']} degrees\")\n",
    "print(f\"   - Parallel processing: {CONFIG['tiling']['parallel']}\")\n",
    "print(f\"   - Use spatial filtering on both Overture and ArcGIS downloads\")\n",
    "print(\"   - Consider smaller chunks if memory issues occur\")\n",
    "\n",
    "print(\"\\n2. File management:\")\n",
    "print(f\"   - Overture GeoParquet: {CONFIG['paths']['overture_data_dir']}\")\n",
    "print(f\"   - ArcGIS + converted FlatGeobuf: {CONFIG['paths']['scratch_dir']}\")\n",
    "print(f\"   - Clean intermediate files between steps if needed\")\n",
    "print(\"   - Use filter patterns to process specific layers only\")\n",
    "\n",
    "print(\"\\n3. Output optimization:\")\n",
    "print(f\"   - PMTiles output: {CONFIG['paths']['tile_dir']}\")\n",
    "print(\"   - Copy final tiles to public directory for web serving\")\n",
    "print(\"   - ArcGIS downloads directly to FlatGeobuf (no conversion needed)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3ee77b",
   "metadata": {},
   "source": [
    "<!-- # Modular Processing Summary\n",
    "\n",
    "This notebook provides a complete, step-by-step approach for **large-scale geospatial data processing** optimized for continent and world-scale datasets.\n",
    "\n",
    "## Core Steps\n",
    "1. **Download Overture Maps data** - Global basemap features using DuckDB (outputs GeoParquet)\n",
    "2. **Download ArcGIS data** (Optional) - Organization-specific layers via REST API (outputs FlatGeobuf)\n",
    "3. **Check and validate** downloaded files from both sources\n",
    "4. **Convert to FlatGeobuf** - Optimize Overture GeoParquet for efficient tiling\n",
    "5. **Generate PMTiles** - Convert all FlatGeobuf files to web-optimized vector tiles\n",
    "6. **Create TileJSON metadata** - Generate metadata for web mapping integration\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### Overture Maps (Global Open Data)\n",
    "- Buildings, roads, water, land use, places, infrastructure\n",
    "- Downloaded as GeoParquet via DuckDB queries\n",
    "- Continent and world-scale processing capability\n",
    "\n",
    "### ArcGIS Feature Server (Organization Data)\n",
    "- Custom organizational layers (boundaries, settlements, facilities)\n",
    "- Downloaded directly via REST API with automatic pagination\n",
    "- Spatial and attribute filtering supported\n",
    "- Outputs directly to FlatGeobuf for tiling\n",
    "\n",
    "## Format Workflow (Optimized for Scale)\n",
    "\n",
    "```\n",
    "Overture Maps (DuckDB)          ArcGIS REST API\n",
    "─────────────────────           ───────────────\n",
    "GeoParquet (.parquet)           FlatGeobuf (.fgb)\n",
    "        ↓                              ↓\n",
    "    Convert                        [Ready]\n",
    "        ↓                              ↓\n",
    "FlatGeobuf (.fgb)  ←────────────────────┘\n",
    "        ↓\n",
    "Tippecanoe Tiling\n",
    "        ↓\n",
    "PMTiles (.pmtiles)\n",
    "        ↓\n",
    "    Web Maps\n",
    "```\n",
    "\n",
    "## Why This Workflow?\n",
    "\n",
    "### 1. GeoParquet for Overture Downloads\n",
    "- **Compact storage**: 50-80% smaller than GeoJSON\n",
    "- **Fast DuckDB queries**: Efficient spatial filtering\n",
    "- **Columnar format**: Excellent compression\n",
    "\n",
    "### 2. FlatGeobuf for Tiling\n",
    "- **Streaming capability**: Process datasets larger than RAM\n",
    "- **Spatial indexing**: R-tree for fast spatial queries\n",
    "- **Native tippecanoe support**: No conversion overhead\n",
    "- **Optimal for large scale**: Tested on continent/world datasets\n",
    "- **Direct from ArcGIS**: Skip conversion step entirely\n",
    "\n",
    "### 3. PMTiles for Serving\n",
    "- **Cloud-native**: Works with any static file host\n",
    "- **Efficient delivery**: HTTP range requests\n",
    "- **No tile server needed**: Direct browser access\n",
    "\n",
    "## Performance Benefits\n",
    "- **Memory efficiency**: Process billions of features without OOM errors\n",
    "- **Disk space**: GeoParquet + FlatGeobuf = 2-3x less than GeoJSON workflow\n",
    "- **Processing speed**: 20-40% faster tile generation vs GeoJSON\n",
    "- **Parallel processing**: Multi-threaded for optimal CPU utilization\n",
    "- **Direct API access**: ArcGIS data downloads directly to optimal format\n",
    "\n",
    "## Scale Capabilities\n",
    "- ✓ **City-scale**: Brooklyn, Paris, Tokyo, Kinshasa\n",
    "- ✓ **Province-scale**: Haut-Lomami, Tanganyika, multiple health zones\n",
    "- ✓ **Country-scale**: DRC, USA, India  \n",
    "- ✓ **Continent-scale**: Africa, Europe, Americas\n",
    "- ✓ **World-scale**: Global basemaps with billions of features\n",
    "\n",
    "## Key Features\n",
    "- **Modular design** - Each step can be run independently\n",
    "- **Multiple data sources** - Combine Overture and organizational data\n",
    "- **Flexible configuration** - Easy to customize for different areas and data types\n",
    "- **Interactive development** - Run steps individually for debugging\n",
    "- **Performance optimized** - Format selection based on dataset size\n",
    "- **Production ready** - Robust error handling and validation\n",
    "- **Memory conscious** - Streaming workflows prevent OOM errors\n",
    "\n",
    "## Output Files\n",
    "Each step generates specific outputs:\n",
    "- **GeoParquet files (.parquet)** - Overture Maps download format\n",
    "- **FlatGeobuf files (.fgb)** - Optimized tiling input (streaming, indexed)\n",
    "- **PMTiles files (.pmtiles)** - Efficient web mapping output\n",
    "- **TileJSON metadata** - MapLibre GL JS integration\n",
    "\n",
    "## Usage Patterns\n",
    "- **Development**: Run steps individually for testing and debugging\n",
    "- **Production**: Execute all steps in sequence for automated processing\n",
    "- **Customization**: Modify CONFIG settings and layer lists to customize data\n",
    "- **Integration**: Use generated PMTiles with web mapping applications\n",
    "\n",
    "## Best Practices for Large Datasets\n",
    "1. **Use spatial filtering**: Apply extent to both Overture and ArcGIS downloads\n",
    "2. **ArcGIS direct to FGB**: Downloads directly to FlatGeobuf (skip conversion)\n",
    "3. **Convert Overture to FGB**: Always convert GeoParquet before tiling\n",
    "4. **Use parallel processing**: Multi-file datasets process faster\n",
    "5. **Monitor disk space**: Keep parquet as source, FGB for tiling\n",
    "6. **Clean up intermediate files**: After successful tiling if needed\n",
    "7. **Process by region**: For extremely large datasets, split by area -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
